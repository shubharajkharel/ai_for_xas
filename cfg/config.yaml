# compound_name: Ti-O
compound_name: Cu-O
max_epochs: 100
data_size: null # 500000
batch_size: 128 # 131072 # 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, 524288, .. 33554432
num_workers: 0 # setting this high slows things down in mac # TODO
devices: [0] # freed up in saturn  1, 4, 5, kill -9 $(lsof -t -i:6006)6

optimization:
  run_distributed: false
  n_trials: 1
  timeout: null # 36000
  n_jobs: 1
  min_depth: 2
  max_depth: 2
  min_width: 100
  max_width: 100

study:
  _target_: optuna.create_study
  study_name: "xas"
  # storage: sqlite:///${study.study_name}.db # currently sqlite
  direction: minimize
  # directions: [minimize, minimize] # or   # direction: minimize
  load_if_exists: False
  sampler: null # None mean /NSGAII for single/mult-objective
  # pruner: not useful for multi-objective
  # _target_: optuna.pruners.MedianPruner

model:
  input_size: 64
  output_size: 200
  loss:
    _target_: torch.nn.MSELoss
    # _target_: src.loss_functions.PulseMaskedMSE

trainer:
  _target_: pytorch_lightning.Trainer
  max_epochs: ${max_epochs}
  devices: ${devices}
  log_every_n_steps: 5
  accelerator: "mps"
  # profiler: simple
  benchmark: True
  precision: "16-mixed"
  callbacks:
    # - _target_: pytorch_lightning.callbacks.early_stopping.EarlyStopping
    #   monitor: val_loss
    #   min_delta: 0.00001
    #   verbose: true
    - _target_: utils.src.lightning.pl_log_callbacks.TensorboardLogTestTrainLoss
    # - _target_: utils.src.lightning.pl_log_callbacks.TensorboardLogAllWeigthsHist
    - _target_: pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint
      monitor: val_loss
      filename: "{epoch:02d}-{val_loss:.2f}"
    # - _target_: pytorch_lightning.callbacks.progress.RichProgressBar
    # - _target_: pytorch_lightning.callbacks.model_summary.ModelSummary
    # - _target_: optuna.integration.PyTorchLightningPruningCallback # not supported for multi objective
    #   monitor: val_loss

data_module:
  _target_: src.dataset.XASDataModule
  compound: ${compound_name}
  split: material-splits
  task: train
  # _target_: utils.src.lightning.pl_data_module.PlDataModule
  # # train_size: 0.7
  # # val_size: 0.15
  random_seed: 42
  batch_size: ${batch_size}
  num_workers: ${num_workers}
  # persistent_workers: true
  # use_cache: False
  # stratify: False # TODO: prestratify
  pin_memory: True
  # prefetch_factor: 4
  # dataset:

hydra:
  job:
    name: ${study.study_name}
    chdir: False
    # config:
    #   hydra.job.override_dirname:
    #     exclude_keys:
    #     - "seed"  # doesnt make seed part of dirname even in multirun
  sweep:
    dir: ./multirun/${hydra.job.name}
    # subdir: ${hydra.job.num}
    subdir: ${hydra.job.override_dirname}
  run:
    dir: ./outputs/${hydra.job.name}/${now:%Y-%m-%d}/${now:%H-%M-%S}/${hydra.job.override_dirname}
